<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<title>笔杆检测报告单（全文标明引文）</title>
	<meta name="keywords" content="" />
	<meta name="description" content="" />
          <meta content="0" http-equiv="Expires"/>
          <meta content="no-cache" http-equiv="Pragma"/>
          <meta content="no-cache" http-equiv="Cache-Control"/>
          <meta content="no-cache" http-equiv="Cache"/>
	<link href="css/report.css?v20180524" type="text/css" rel="stylesheet" />
	<script src="js/jquery.tools.pack.js" type="text/javascript"></script>
	<script type="text/javascript">
    function $ShowMore(n) {
        if ($("#simMore_" + n + " a").text() == '收起相似文献') {//收起
            $("#reportTable_" + n + " .trLike").hide();
            for (var i = 0; i < 5; i++) {
                $("#reportTable_" + n + " .trLike:eq("+i+")").show();
            }
            $("#simMore_" + n + " a").html('查看更多相似文献<span class="icons inlineBlock simDown"></span>');
        } else {
            $("#reportTable_" + n + " .trLike").show();
            $("#simMore_" + n + " a").html('收起相似文献<span class="icons inlineBlock simUp"></span>');
        }
}</script>
<style>
    em.similar{color:Red; font-style:normal;}
</style>
</head>
<body>
<div class="report_bg2">
  <div class="report_bg3">
    <div class="report_top">
      
      <h1>笔杆检测报告单<span>（全文标明引文）</span></h1>
    </div>
    <div class="report_Wrap">
      <div class="report_tab" id="report_tab">
       <ul>
                                            <li class="rep_curr"><div><a href="全文标明引文" class="green">全文标明引文</a></div></li>
                                            <li><div><a href="全文对照.html" class="green">全文对照</a></div></li>
        </ul>
        <div class="report_priSav">
          <a href="javascript:window.print();" class="print inlineBlock"><span class="icons inlineBlock"></span>打印</a>
          <a target="_blank" href="https://www.bigan.net/report/explain.html" class="report_explain inlineBlock"><span class="icons inlineBlock"></span>检测说明</a>
        </div>
      </div>
      <div class="report_content">
        <div class="report_main">
          <a id="toTop" title="回到顶部"></a><!-- 回到顶部 -->
          <script>
              $(document).ready(function () {
                  $("#toTop").hide();
                  //检测屏幕高度
                  var height = $(window).height();
                  //scroll() 方法为滚动事件
                  $(window).scroll(function () {
                      if ($(window).scrollTop() > height) {
                          $("#toTop").fadeIn(500);
                      } else {
                          $("#toTop").fadeOut(500);
                      }
                  });
                  $("#toTop").click(function () {
                      $('body,html').animate({ scrollTop: 0 }, 100);
                      return false;
                  });
              });
          </script>
           <div class="report_Mtop"></div>
          <div class="report_Mbot">
            <div class="report_result">
              <div class="report_info">
                <p><span>标题：</span>Supervised Learning for Retinal Disease Classification using Fundus Images .pdf</p>
                <p><span>作者：</span>Gu Weixuan</p>
                <p><span>报告编号：</span>BG202505042027540441</p>
                <p><span>提交时间：</span>2025-05-04 20:28:33</p>
              </div>
              <div class="report_ratio">
                <ul>
                  <li style="display:none;"><span class="icons ratioIcon ratio1"></span>总复制比：<span class="green">5.3%</span></li>
                  <li class="inlineBlock"><span class="icons ratioIcon ratio2"></span>去除引用文献复制比：<span class="green">5.1%</span></li>
                  <li class="inlineBlock"><span class="icons ratioIcon ratio3"></span>去除本人已发表文献复制比：<span class="green">5.3%</span></li>
                  <li class="inlineBlock"><span class="icons ratioIcon ratio4"></span>单篇最大文字复制比：<span class="green">2.7%</span></li>
                </ul>
              </div>
               <div class="clear"></div>
              <div class="seal">
                <div class="SealArea">
                  <div class="SealBg"><img src="data:image/png;base64,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" /></div>
                  <div class="SealPercent"></div>
                </div>
              </div>
            </div>
            <div class="repeat_words">
              <div class="repWords_box1">
                <div class="repWords_row">
                  <div class="test_range"><a href="https://www.bigan.net/qa/?key=%E6%A3%80%E6%B5%8B%E8%8C%83%E5%9B%B4" target="_blank" class="green"><span class="icons inlineBlock"></span>检测范围</a></div>
                  <span>重复字数:<b class="red">3,319</b></span><span>总字数：<b>62,856</b></span> </div>
                <div class="clear"></div>
              </div>
              </div>
              <div class="clear"></div>
              <div class="similarLiter">

                <h2 style="width:100%;text-align:center;margin-top:15px;">Supervised Learning for Retinal Disease Classification using Fundus Images .pdf_第1部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>UNDERGRADUATE PROJECT REPORT</p><p>Project Title:</p><p>Supervised Learning for Retinal Disease Classification using</p><p>Fundus Images</p><p>Surname: Gu</p><p>First Name: Weixuan</p><p>Student Number:202118020326</p><p>Supervisor Name: Dr Grace Ugochi Nneji</p><p>Module Code: CHC 6096</p><p>Module Name: Project</p><p>Date Submitted: May 6,2024</p><p>Chengdu University of Technology Oxford Brookes College</p><p>Chengdu University of Technology</p><p>BSc (Single Honours) Degree Project</p><p>Programme Name: Software Engineering</p><p>Module No.: CHC 6096</p><p>Surname: Gu</p><p>First Name: Weixuan</p><p>Project Title: Supervised Learning for Retinal Disease Classification using Fundus</p><p>Images</p><p>Student No.:202118020326</p><p>Supervisor: Dr. Grace Ugochi Nneji</p><p>Date submitted:6th May 2025</p><p>A report submitted as part of the requirements for the degree of BSc (Hons) in Computer</p><p>Science</p><p>At</p><p>Chengdu University of Technology Oxford Brookes College</p><p>Declaration</p><p>Student Conduct Regulations:</p><p>Please ensure you are familiar with the regulations in relation to Academic Integrity. The</p><p>University takes this issue very seriously and students have been expelled or had their</p><p>degrees withheld for cheating in assessment. It is important that students having</p><p>difficulties with their work should seek help from their tutors rather than be tempted to</p><p>use unfair means to gain marks. Students should not risk losing their degree and</p><p>undermining all the work they have done towards it. You are expected to have familiarised yourself with these regulations.</p><p>https://www.brookes.ac.uk/regulations/current/appeals-complaints-and-conduct/c1-1/</p><p>Guidance on the correct use of references can be found on www.brookes.ac.uk/services/library, and also in a handout in the Library.</p><p>The full regulations may be accessed online at</p><p>https://www.brookes.ac.uk/students/sirt/student-conduct/</p><p>If you do not understand what any of these terms mean, you should ask your Project Supervisor to clarify them for you.</p><p>I declare that I have read and understood Regulations C1.1.4 of the Regulations governing Academic Misconduct, and that the work I submit is fully in accordance with them.</p><p>Signature Date 6th May 2025</p><p>REGULATIONS GOVERNING THE DEPOSIT AND USE OF OXFORD BROOKES</p><p>UNIVERSITY MODULAR PROGRAMME PROJECTS AND DISSERTATIONS</p><p>Copies of projects/dissertations, submitted in fulfillment of Modular Programme requirements and achieving marks of 60% or above, shall normally be kept by the Oxford Brookes University Library.</p><p>I agree that this dissertation may be available for reading and photocopying in accordance with the Regulations governing the use of the Oxford Brookes University Library.</p><p>Signature Date 6th May 2025.</p><p>Acknowledgment</p><p>The author would like to express heartfelt gratitude to his professors, friends, and family for their continuous and firm support, encouragement, and precious advice throughout the preparation of this project. Special thanks go to Tina, Jojo, and Grace, whose insightful suggestions and thoughtful feedbacks at every stage of the project were indispensable in guiding the direction and ensuring the succussed of completion. Furthermore, the author wishes to express gratitude to Grace particularly for her meticulous guidance and patience, which helped the author overcome numbers of difficulties and refine the project. Her expertise and thoughtful guidance provided clarity and direction during the most challenging moments. The author also extends his deepest gratitude to the members of the school council from both CDUT and Oxford Brooks for their constructive input, which provides a lot of resources to helped refine and elevate the quality of this work. Furthermore,<em class='similar'> the encouragement and understanding from the author’s family and friends have been a constant source of strength and motivation,</em> allowing him to persevere through the rigorous demands of this project. Without their support, both emotional and intellectual, this project would not have been possible. The author is deeply grateful for their unwavering belief in his abilities and for being there through every step of this journey.</p><p>Table of Contents</p><p>Declaration . ii</p><p>Acknowledgment . iii</p><p>Table of Contents . iv</p><p>List of Figures . vi</p><p>List of Tables. viii</p><p>Abstract .ix</p><p>Abbreviations . x</p><p>Glossary .xi</p><p>Chapter 1 Introduction .1</p><p>1.1 Background.1</p><p>1.1.1 Risk and Factor .1</p><p>1.1.2 Challenge .3</p><p>1.2 Aim .3</p><p>1.3 Objectives .4</p><p>1.4 Project Overview .5</p><p>1.4.1 Scope .5</p><p>1.4.2 Audience .5</p><p>1.4.3 Project Overview .6</p><p>Chapter 2 Background Review .7</p><p>2.1 Early DR detection means .7</p><p>2.2 Machine learning techniques for DR Detection .7</p><p>2.3 Deep learning methods for detecting DR .8</p><p>2.3.1 Supervised learning Techniques .8</p><p>2.3.2 Self-supervised learning Techniques .9</p><p>2.3.3 Other Researches on Generative Adversarial Network (GAN) and Attention</p><p>Networks .10</p><p>Chapter 3 Methodology .14</p><p>3.1 Approach .14</p><p>3.2 VGG .14</p><p>3.3 ResNet .16</p><p>3.4 Self-Supervised Learning and Supervised Learning .18</p><p>3.5 Squeeze-and-Excitation attention .18</p><p>3.6 Evaluation Metrics .19</p><p>3.7 Dataset .20</p><p>3.8 Dataset preprocessing .21</p><p>3.8.1 Image Cropping and Enhancement .21</p><p>3.8.2 Dataset Balancing .22</p><p>Chapter 4 Implementation and Result Analyses .25</p><p>4.1 The implementation of the Gu_CNN model .25</p><p>4.2 Initial Convolution and Residual Feature Learning .25</p><p>4.3 Stage-wise Architecture Details .26</p><p>4.4 Global Pooling and Classification .28</p><p>4.5 Implementation Details .28</p><p>4.6 Result .30</p><p>4.7 Self-Supervised Model .34</p><p>4.7.1 Differences in Self-Supervised model with Supervised Model .34</p><p>4.7.2 Self-Supervised Model Results .35</p><p>4.8 Result comparation .38</p><p>4.9 Model Visualization .38</p><p>4.10 GUI .39</p><p>Chapter 5 Professional Issues .42</p><p>5.1 Project Management.42</p><p>5.1.1 Activities .42</p><p>5.1.2 Schedule.45</p><p>5.1.3 Project Data Management .45</p><p>5.1.4 Project Deliverables .45</p><p>5.2 Risk Analysis .45</p><p>5.3 Professional Issues .46</p><p>Chapter 6 Conclusion .48</p><p>References .49</p><p>List of Figures</p><p>Figure 1 The neurovascular Unit of the Retina[4].2</p><p>Figure 2 The five different DR stages increase in severity from left to right: no DR,</p><p>mild DR, moderate DR, severe DR, and proliferative DR.[5].2</p><p>Figure 3 Project overview .6</p><p><em class='similar'>Figure 4 Natural history of diabetic retinopathy based on retinal microvascular</em></p><p>disease progression and current treatment options.[12].7</p><p>Figure 5 Architecture of a Typical method of ML (Random Forest)[14].8</p><p>Figure 6 Topology of Classical CNN-FC [15].8</p><p>Figure 7 Pre-trained Supervised Learning Model .9</p><p>Figure 8 Pre-trained Self-supervised learning model .10</p><p>Figure 9 Architecture of VGG19.15</p><p>Figure 10 Architecture of ResNet50.17</p><p>Figure 11 Basic module of Residual learning [25].17</p><p>Figure 12 Structure of SE module[27].18</p><p>Figure 13 Example images from the APTOS 2019 dataset:.21</p><p>Figure 14 Comparison of image cropping and enhancement results .22</p><p>Figure 15Comparison of dataset distribution before and after expansion .24</p><p>Figure 16 The general architecture diagram of the model .25</p><p>Figure 17 Architecture of the Residual Block with SE Attention .26</p><p>Figure 18 Confusion Matrix of Gu_CNN on Aptos2019 dataset .31</p><p>Figure 19 ROC curves of Gu_CNN on Aptos2019 dataset .32</p><p>Figure 20 Precision-Recall curves of Gu_CNN on Aptos2019 dataset.32</p><p>Figure 21 Training and Validation Loss(up) and Accuracy(down) curvesof Gu_CNN</p><p>on Aptos2019 dataset .33 Figure 22 Training and Validation Loss(up) and Accuracy(down) curves of Self-</p><p>supervised model on Aptos2019 dataset .36</p><p>Figure 23 Confusion Matrix of Self-supervised model on Aptos2019 dataset .37</p><p>Figure 24 Precision-Recall curves of Self-supervised model on Aptos2019 dataset</p><p>.37</p><p>Figure 25 ROC curves of Self-supervised model on Aptos2019 dataset .38</p><p>Figure 26 Grad-CAM Visualization .39</p><p>Figure 27 Home Page .40</p><p>Figure 28 Dataset shown in home page .40</p><p>Figure 29 DR Check page .41</p><p>Figure 30 Gantt Chart .45</p><p>viii</p><p>List of Tables</p><p>Table 1 Summary of Related Works .11</p><p>Table 2 Overview of images from the APTOS 2019 dataset .20</p><p>Table 3 Architecture Overview of the Gu_CNN Model Across Different Stages .27</p><p>Table 4 Summary of Hyperparameters .29</p><p>Table 5 Classification Report .30</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">Supervised Learning for Retinal Disease Classification using Fundus Images .pdf_第2部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>Table 6 Classification Report for Self-Supervised Model .35</p><p>Table 7 Table of Activities .42</p><p>ix</p><p>Abstract</p><p><em class='similar'>Diabetes is a chronic disease which can lead to serious complications,</em><em class='similar'> including the diabetic retinopathy </em><em class='similar'>(DR).</em><em class='similar'> DR is a severe illness and it is the main cause of blindness among people who are under the age of 50.</em> DR happens when long-term high blood sugar level damaged the retinal blood vessels, and that will lead to swelling, fluid leakage, and finally vision impairment. In this situation, early detection and accurate classification of DR are important for early effective intervention. This project introduces Gu_CNN model, a lightweight convolutional neural network (CNN) designed for the classification of the severity of DR&#39;s condition. The model combines VGG-style convolutional layers for early visual feature extraction, residual blocks of ResNet for deep feature learning, and Squeeze-and-Excitation (SE) attention modules to improve channel feature recalibration. The training was done on the APTOS 2019 Blindness Detection dataset, all the pictures have gone through structured preprocessing before use, such as image normalization, data augmentation and class rebalancing. As the result, the model achieved a test accuracy of 89.80% and a macro F1-score of 0.8988. Additional evaluation using confusion matrix analysis, ROC curves, and Precision-Recall curves confirmed the model&#39;s robustness and stable convergence. This work demonstrates the potential of combining classical feature extraction with attention mechanisms to build efficient and interpretable models for DR classification. Future research may focus on enhancing generalization performance under limited data conditions and further reducing loss values to improve accuracy.</p><p>Keywords: Diabetic Retinopathy, Deep Learning, Convolutional Neural Networks, Residual Learning, Attention Mechanism, Medical Image Classification, Squeeze-and-</p><p>Excitation</p><p>x</p><p>Abbreviations</p><p>DR: Diabetic Retinopathy</p><p>CNN: Convolutional Neural Network</p><p>SE: Squeeze-and-Excitation</p><p>VGG: Visual Geometry Group</p><p>ResNet: Residual Neural Network</p><p>APTOS: Asia Pacific Tele-Ophthalmology Society</p><p>ML: Machine Learning</p><p>DL: Deep Learning</p><p>GAN: Generative Adversarial Network OCT: Optical Coherence Tomography</p><p>ROC: Receiver Operating Characteristic</p><p>AUC: Area Under the Curve</p><p>PR: Precision-Recall</p><p>GAP: Global Average Pooling</p><p>ReLU: Rectified Linear Unit</p><p>BN: Batch Normalization</p><p>TP: True Positive</p><p>TN: True Negative</p><p>FP: False Positive</p><p>FN: False Negative</p><p>xi</p><p>Glossary</p><p>This section defines the keywords and terminology from the Abstract and other relevant</p><p>parts of the report:</p><p>Diabetic Retinopathy (DR): A diabetes complication affecting the retina, leading to vision impairment or blindness due to damaged blood vessels.</p><p><em class='similar'>Convolutional Neural Network </em><em class='similar'>(CNN):</em><em class='similar'> A deep learning model designed for image processing,</em> using convolutional layers to extract hierarchical features.</p><p>Residual Learning: A technique in deep learning where skip connections (residual blocks) mitigate gradient vanishing, enabling deeper networks (e.g., ResNet).</p><p>Attention Mechanism: A module (e.g., Squeeze-and-Excitation) that dynamically</p><p>recalibrates channel-wise feature responses to enhance model focus on relevant patterns.</p><p>Macro F1-Score: The unweighted average of F1-scores across all classes, used to evaluate model performance on imbalanced datasets.</p><p><em class='similar'>Fundus Images:</em><em class='similar'> Photographs of the interior surface of the eye,</em><em class='similar'> including the retina,</em> used for diagnosing DR.</p><p>Data Augmentation (DA): Techniques (e.g., rotation, scaling) to artificially expand training data and improve model generalization.</p><p>Gradient Clipping: A method to prevent exploding gradients by capping gradient values during backpropagation.</p><p>Class Rebalancing: Adjusting sample weights or oversampling minority classes to address dataset imbalance.</p><p>Note: Common terms like &quot;accuracy,&quot;&quot;loss,&quot; or &quot;dataset&quot; are omitted as they are standard in computer science. Definitions align with the report’s technical context and aim to clarify domain-specific jargon.</p><p>Chapter 1 Introduction</p><p>1.1 Background</p><p><em class='similar'>Diabetes is a chronic disease.</em><em class='similar'> Its pathogenesis is that the pancreas cannot produce or secrete enough insulin to control blood sugar,</em><em class='similar'> which leads to elevated blood sugar levels.</em><em class='similar'> This disease is not only widespread in society,</em> but also poses a high health risk.<em class='similar'>[1]</em><em class='similar'> One of the most severe complications is diabetic retinopathy </em><em class='similar'>(DR),</em><em class='similar'> which is the leading cause of blindness among people under the age of 50.</em> In DR,<em class='similar'> consistently high blood sugar damage the retinal blood vessels,</em><em class='similar'> leading to swelling and leakage of blood or fluid.</em> The most harmful vision loss typically occurs when the central retina swells, and this will</p><p>seriously affect the patient&#39;s daily life.[2][3]</p><p>1.1.1 Risk and Factor</p><p>As far as the pathogenesis of DR is concerned, diabetes contributes directly to the development of DR by causing vascular damage in the retina. These damages mainly occur in the Retinal ganglion cells, Photoreceptor cells, Endothelial cells and Pericytes shown in Figure 1. Uncontrolled blood glucose is a major risk factor, i.e., chronic high blood glucose levels due to insufficient insulin production or secretion initiate this destructive process.[2] In short, diabetes-induced retinal dysfunction can be viewed as damage to the retinal neurovascular unit.[4][5].</p><p>Figure 1 The neurovascular Unit of the Retina[4]</p><p>DR begins with retinal injury without visible microvascular abnormalities, small changes in the blood vessels (Non-proliferative retinal microvascular changes such as microaneurysms and punctate intraretinal hemorrhages), which is referred to as mild DR. At this stage, full recovery is possible. If proper care is not taken, after a few years, it progresses to moderate DR, at which point vascular leakage may begin. If not given adequate attention, pre-proliferative changes characterized by the presence of &quot;cotton wool&quot; spots will occur, and the disease will further progress to severe DR. Worse, it can evolve into the appearance of proliferative DR where abnormal neovascularization occurs, and can lead to irreversible visual impairment or even permanent blindness, especially if</p><p>the swelling involves the central retina.[4][5]</p><p>Figure 2 The five different DR stages increase in severity from left to right: no DR, mild</p><p>DR, moderate DR, severe DR, and proliferative DR.[5]</p><p>Even more worrisome is the global burden of diabetes is <em class='reference'>increasing. According to Cho et al., there were about 451 million people aged 18-99 with diabetes in 2017, a number projected to increase to 693 million by 2045. This upward </em>trend indicates that the number of patients at risk for DR will likewise grow, underscoring the importance of early detection</p><p>and intervention.[5]</p><p>1.1.2 Challenge</p><p>While machine learning (ML) and deep learning (DL) show potential in medical image analysis, their practical application in the identification of diabetic retinopathy (DR) still</p><p>faces the following key challenges:</p><p>The development of DR Model is highly dependent on standardized retinal image data, but there are generally problems in the actual data set, such as unbalanced category distribution (such as the scarcity of advanced cases), inconsistent expert labeling (different physicians have significant differences in judging the stage of lesions), and feature variation caused by multi-source imaging equipment. These data limitations may lead to model training bias, reduce the detection sensitivity of early lesions and severe cases, and thus affect the reliability of clinical diagnosis.</p><p>In addition, technology deployment faces multiple real-world barriers. The mismatch between hardware computing power needs and the resources of primary care institutions, the compatibility of algorithms with hospital information systems (such as PACS, electronic medical records), and the lack of a standardized regulatory framework for AI-assisted DR Diagnosis. These systemic challenges keep most studies in the experimental</p><p>stage and make it difficult to achieve large-scale clinical response.[2]</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">Supervised Learning for Retinal Disease Classification using Fundus Images .pdf_第3部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>1.2 Aim</p><p>As VGG has deeper depth and denser convolutional layers, however it is prone to the problem of gradient vanishing, while ResNet can effectively reduce the gradient vanishing problem through residual connections. In this study, it is tried to optimize the model by using the first few layers of VGG for extracting basic features, and then combining it with ResNet&#39;s residual structure for extracting advanced features to take advantage of their strengths and overcome the weaknesses. In addition, in the paper of S. Mishra et al.[7] it is mentioned that the pre-trained model has a greater improvement compared to the normal VGG16, in this study we will also try to pre-train the combined model and check</p><p>the final result.[6]</p><p>1.3 Objectives</p><p>The project will propose a supervised CNN model (combining VGG and ResNet) for the detection of DR and <em class='similar'>will be based on the dataset provided in the Asia Pacific Tele-Ophthalmology Society </em><em class='similar'>(APTOS)</em><em class='similar'> competition ‘APTOS 2019 Blindness Detection’ competition organized by the Asia Pacific Tele-Ophthalmology Society </em><em class='similar'>(APTOS)</em> on</p><p>Kaggle website.[a]</p><p>The author will segment, preprocess and enhance the dataset before training to improve the accuracy of the experimental results. After obtaining the best and stable experimental results, this project will compare and analyze the data from different directions. The evaluation of the models built in this project will be based on ‘Accuracy’,‘Loss’,‘Precision’,‘Recall ‘and ‘F1-Score’, and adhere to a rigorous scientific attitude to draw the corresponding conclusions.</p><p>1.4 Project Overview</p><p>This subsection will explain what the significance of this project is and what benefits it can bring from both Scope and Audience perspectives. The general flow of the project will also be shown.</p><p>1.4.1 Scope</p><p>The application of Convolutional Neural Networks (CNNs) in medical imaging, particularly for diabetic retinopathy (DR) detection, faces several challenges though it succeed in general image classification.[7] While VGG19 combined with GAN has shown great use in improving accuracy and reducing dataset requirements,[8] VGG19’s depth often leads to gradient vanishing, hindering updates to shallow-layer features and overall learning efficiency. Additionally, the model&#39;s high parameter count and complexity can cause overfitting and make it computationally demanding. To solve these limitations, this project proposes a CNN model that introduces residual learning to VGG by combining ResNet to improve network performance.[9][10] This reduces parameters through depth wise convolutions and incorporates skip connections to enhance information flow, thereby improving efficiency and feature capture.</p><p>The significance of this study is:</p><p>⚫ Contribute to the medical field</p><p>⚫ Reduce training costs</p><p>⚫ Help poor countries save money</p><p>⚫ Improve efficiency in detecting whether a DR diagnosis is confirmed or not</p><p>⚫ Reduce the rate of misdiagnosis</p><p>⚫ Reduce stress for the healthcare professionals involved</p><p>⚫ More efficient and accurate categorization of DR conditions</p><p>1.4.2 Audience</p><p>The beneficiaries of the project are as follows:</p><p>◆ Healthcare professionals: Ophthalmologists will benefit from increased accuracy and</p><p>efficiency in DR diagnosis, reducing repetitive works and significantly reducing their workload.</p><p>◆ Hospitals: Streamlining the diagnostic process, reducing overall overheads, improve</p><p>diagnostic efficiency and reduce misdiagnosis rates, as a result to improve patient care and outcomes, and indirectly reducing the likelihood of patient-physician conflict due to misdiagnosis.</p><p>◆ DR patients: Benefiting from the project&#39;s models, patients can be diagnosed faster</p><p>and more accurately, reducing the incidence of letting their condition worsen and lowering the risk of blindness.</p><p>◆ Machine learning researchers: future researchers can continue to propose more</p><p>valuable and better models based on this project, promoting the lasting development of machine learning.</p><p>1.4.3 Project Overview</p><p>The project primarily revolves around using a deep learning model to diagnose diabetic retinopathy (DR). First, the author retrieves data from online datasets and performs preprocessing, which includes data enhancement, sampling, and label reassignment. The processed data is then used to train the initial model, and it will be regularly updated as new and better datasets become available later. The trained model is deployed in a graphical user interface (GUI), where users can upload fundus images and get the prediction results from the system. Additionally, the system ensures user privacy by not saving uploaded images and focusing solely on analyzing and predicting results. Besides, it is important to let the user note that the prediction results provided by the system are for reference purposes only and cannot be the replacement of a real human doctor&#39;s</p><p>diagnosis. The flowchart of the project overview is shown in Figure 3:</p><p>Figure 3 Project overview</p><p>Chapter 2 Background Review</p><p>This section will discuss the contributions made by other researchers in identifying DF, and provide a list comparing their models.</p><p>2.1 Early DR detection means</p><p>Previously, detection of DR mostly relied on traditional ophthalmological methods, such as direct observation of the retina or the use of fundus photography, as well as focusing on the blood sugar levels. These methods allowed doctors to identify lesions by looking at physical changes in the retina, or changes in blood composition. As technology advanced, optical coherence tomography (OCT) was introduced, providing more detailed images of the retinal structure and helping to detect wounds earlier. However, these traditional methods have limitations, such as being highly dependent on the experience</p><p>and skill of the doctor, and may not be sensitive enough to identify early lesions.[11]</p><p><em class='similar'>Figure 4 Natural history of diabetic retinopathy based on retinal microvascular disease</em></p><p>progression and current treatment options.[12]</p><p>2.2 Machine learning techniques for DR Detection</p><p><em class='similar'>In later years,</em><em class='similar'> machine learning techniques began to be applied to the detection of DR.</em><em class='similar'> The machine learning algorithms,</em><em class='similar'> for example like support vector machines </em><em class='similar'>(SVMs)</em><em class='similar'> and random forests,</em> were used to analyze fundus images to check for any early signs of DR. These algorithms were trained on a large number of retinal images and were able to detect lesions with some sensitivity and specificity. However, while traditional machine learning methods can only just possible to reach parity with a human clinician, ML detection may require more manual design for feature extraction and may have limitations in dealing with</p><p>complex feature classes.[13]</p><p>Figure 5 Architecture of a Typical method of ML (Random Forest)[14]</p><p>2.3 Deep learning methods for detecting DR</p><p>The advent of deep learning techniques, especially CNNs, has revolutionized DR detection. CNNs are able to automatically learn complex feature representations from images, improving the accuracy of detection.[14] As some of the models shown in Table 1 above, they all show considerable accuracy, sensitivity and specificity. However, it is worth noting book that the vast majority of DR&#39;s datasets currently have some imbalance problem, which may lead to the models encountering some problems during the training process, such as Loss is too high as well as the accuracy cannot be improved.</p><p>Figure 6 Topology of Classical CNN-FC [15]</p><p>2.3.1 Supervised learning Techniques</p><p>In supervised learning, model learns a prediction or classification task from training data with known inputs and corresponding outputs. This approach relies on a large amount of labelled data so that the model can learn the mapping relationship between input features and output labels.</p><p>The application of supervised learning to DR detection includes the use of deep learning models such as CNNs that are trained on a large number of labelled fundus images to identify lesion features and perform classification. The advantage of this approach is that the models can learn accurate feature representations, but the disadvantage is that it requires a large amount of labelling work, which is particularly challenging in the field of</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">Supervised Learning for Retinal Disease Classification using Fundus Images .pdf_第4部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>medical images</p><p>Figure 7 Pre-trained Supervised Learning Model</p><p>2.3.2 Self-supervised learning Techniques</p><p>Self-supervised learning is a special type of unsupervised learning in which models are trained by self-generating labels. In other words, the model uses the input data itself to create labels and then uses these labels to learn.</p><p>In DR detection, self-supervised learning can be used to generate internal labels by designing specific auxiliary tasks, such as pre-training an encoder with a contrast learning algorithm and then re-training the encoder and classifier on a small amount of labelled data. The advantage of this approach is that it can reduce the dependence on a large amount of labelled data, which helps to solve the problem of insufficient labelled data for medical images. Self-supervised learning models, such as SimCLR-DR, have shown the ability to overcome the problem of insufficient training data in DR detection and outperform</p><p>traditional migration learning</p><p>Figure 8 Pre-trained Self-supervised learning model</p><p>2.3.3 Other Researches on Generative Adversarial Network (GAN) and Attention</p><p>Networks</p><p>Vaish et al.[16] used an improved ResNet-50 deep learning model to classify diabetic retinopathy. They used an improved pre-trained ResNet-50 model to develop a deep model and used the Adam optimizer to accelerate the convergence process. Their model achieved an accuracy of 74.18%, sensitivity of 85.08%, specificity of 87.50% and F1-score of 81.35% on the Kaggle APTOS dataset.</p><p>The approach adopted by Kabilan et al.[17] is to combine Deep Convolutional Generative Adversarial Network (DCGAN) with VGG19. Their idea is to perform preprocessing of retinal images through Contrast Constrained Adaptive Histogram Equalisation (CLAHE) technique where the contrast features of the image are designed to be identified from the original retinal image to identify clear vascular structures, which in turn helps the DCGAN to synthesise the image and finally allows VGG19 to produce accurate results. The results of their model on the unmentioned Dataset are 88.93% Accuracy,89.68% Sensitivity,88.31% Precision,86.72% F1 Score, and 85.31% Recall, while the results obtained using SVM on the same Dataset were 84.19%,83.1%,85.41%,86.0%, and 84.88%. Same, The result of CNN is 86.23%,84.2%,85.21%,87.29, and 84.68%.</p><p>He et al.[18] proposed a new category attention block (CAB) for unbalanced DR data distributions, which explores more discriminative regional features for each DR class and treats each category equally. To capture more detailed information about small lesions, they also proposed the global attention block (GAB), which exploits detailed and category-independent global attention feature maps of fundus images. They tested it on DDR dataset, Messidor dataset, and EyePACS dataset, and the best test results were in 85.69%,86.68%, and 86.18% accuracy.</p><p>Deshpande et al.[19] used a pre-trained model, Inception V3.<em class='similar'> It was fine-tuned to add a custom classification layer consisting of a GlobalA veragePooling2D layer followed by two fully connected layers with ReLU activation and a SoftMax activation layer.</em><em class='similar'> Their model was trained and tested using two publicly available datasets EyePACS and APTOS 2019.</em> Their model obtained 74.28% and 73.<em class='similar'>81% accuracy and F1 score on the EyePACS dataset and 81.61% and 80.21% accuracy and F1 score on the APTOS 19 dataset,</em></p><p>respectively.[20]</p><p>The CNN model of Matthew et al.[21] was constructed using migration learning from an existing model, EfficientNet-B0, which had been pre-trained using the ImageNet dataset. The base model was then modified by adding tuning to the parameters and adding several new layers to the model. The results show that EfficientNet-B0 has an accuracy of 81.52%,</p><p>but a relatively low F1 score of 67% due to data imbalance. Table 1 summarizes the findings and potential outcomes of the different researchers.</p><p>Table 1 Summary of Related Works</p><p>Author Datasets Methods &amp;</p><p>Models</p><p>Results Limitations</p><p>Vaish et al.</p><p>[16]</p><p>Kaggle</p><p>APTOS</p><p>pre-trained</p><p>ResNet-50,</p><p>Adam optimizer</p><p>accuracy of</p><p>74.18%</p><p>sensitivity of</p><p>85.08%</p><p>specificity of</p><p>87.50%</p><p>F1-score of</p><p>81.35%</p><p>The model&#39;s</p><p>dependence on</p><p>input image</p><p>quality,</p><p>unevenness of</p><p>data set</p><p>categories may</p><p>affect model</p><p>generalization,</p><p>and the improved</p><p>ResNet-50</p><p>model, although</p><p>excellent, may</p><p>still be limited by the optimizer and</p><p>hyperparameter</p><p>selection on the</p><p>training effect.</p><p>Kabilan et</p><p>al.[9]</p><p>Data is</p><p>obtained from</p><p>kaggle and</p><p>been</p><p>processed.</p><p>Dataset name</p><p>is not</p><p>mentioned.</p><p>DCGAN+VGG1988.93%</p><p>Accuracy</p><p>89.68%</p><p>Sensitivity</p><p>88.31%</p><p>Precision</p><p>86.72% F1</p><p>Score</p><p>85.31% Recall</p><p>Relying on GAN</p><p>to generate data</p><p>may affect</p><p>authenticity;</p><p>VGG19 model is</p><p>old and has low</p><p>computational</p><p>efficiency. Actual</p><p>clinical</p><p>applicability has</p><p>not been verified.</p><p>He et al.</p><p>[17]</p><p>DDR,</p><p>Messidor,</p><p>EyePACS</p><p>CABNet on</p><p>mainstream</p><p>CNN</p><p>Architecture</p><p>Accuracy:</p><p>85.69% on DDR,</p><p>86.68% on</p><p>Messidor,</p><p>86.18% on</p><p>EtePACS</p><p>Only image-level</p><p>supervision</p><p>resulted in</p><p>inaccurate</p><p>localization of</p><p>small lesions.</p><p>Inability to</p><p>identify the</p><p>specific lesion</p><p>type (e.g.</p><p>exudation type);</p><p>The</p><p>generalization of</p><p>the model on a</p><p>broader dataset</p><p>was not verified.</p><p>Deshpande</p><p>et al.[18]</p><p>EyePACS.</p><p>APTOS 19</p><p>pre-trained</p><p>model Inception</p><p>V3</p><p>EyePACS:</p><p>Accuracy</p><p>74.28%</p><p>F1 score</p><p>73.81%</p><p>APTOS 19:</p><p>Accuracy</p><p>81.61%</p><p>F1 score</p><p>80.21%</p><p>Sensitivity to</p><p>input image</p><p>quality (such as</p><p>resolution,</p><p>lighting</p><p>conditions, and</p><p>distortion), and</p><p>the impact of</p><p>class imbalances</p><p>and labeling</p><p>errors in the</p><p>EyePACS</p><p>dataset on model</p><p>accuracy and</p><p>reliability of</p><p>results.</p><p>Matthew et</p><p>al.[19]</p><p>APTOS 2019</p><p>Blindness</p><p>Detection</p><p>EfficientNet-B0 Accuracy</p><p>81.52%</p><p>F1 score 67%</p><p>Data imbalance</p><p>leads to poor</p><p>identification of a few classes;</p><p>Relies on</p><p>additional</p><p>hardware (20D</p><p>lens); Merging</p><p>categories</p><p>sacrifices</p><p>diagnostic</p><p>details.</p><p>Chapter 3 Methodology</p><p>3.1 Approach</p><p>The CNN model proposed in this study has a combination of VGG (Similar convolution layer) and ResNet (Use the residual layer). The general idea is to achieve the purpose of combining the two parts by sharing shallow convolutional and branching feature extractors.</p><p>This new model Gu_CNN has two main advantages:</p><p>◆ Reducing the number of parameters: Shared shallow convolution reduces the</p><p>amount of redundant computation in the initial convolutional layers, resulting in a model with fewer parameters than the fully parallel approach.</p><p>◆ Extracting multi-level features: VGG is good at details and ResNet is good at global, through this approach it can achieve the complementary fusion of different features.</p><p>3.2 VGG</p><p>The VGG neural network model is a relatively complete deep neural network model based on CNN. It was constructed by the Computer Vision Group at the University of Oxford.[22]</p><p>As an example, VGG19 is a deep convolutional neural network proposed by the team Visual Geometry Group from the University of Oxford in 2014, which improves the accuracy of image categorization by increasing the number of network layers (a total of 19) and using smaller convolutional kernels (3x3). VGG19, with its simple structure and high performance, has become a classic architecture in deep learning and is especially</p><p>suited to handle large-scale image classification tasks.[23]</p><p>The network model of VGG-19 is shown below:</p><p>Figure 9 Architecture of VGG19</p><p>VGG19 consists of 16 Convolutional Layers (Conv),5 Max Pooling Layers (Max Pooling),3 Fully Connected Layers (Fully Connected) and Activation Functions (ReLU). The main</p><p>features are as follows:</p><p>3.2.1 Convolutional layer:</p><p><em class='similar'>Convolutional layer is used to extract the spatial features of the image.</em><em class='similar'> Assuming that the input feature map size is H×W,</em><em class='similar'> the convolution kernel size is K×K,</em> the output feature map</p><p>size is H′×W′, the calculation formula is:</p><p>𝑯′=</p><p>𝑯−𝑲+𝟐𝑷</p><p>𝑺</p><p>+𝟏(𝟏)</p><p>𝑾′=</p><p>𝑾−𝑲+𝟐𝑷</p><p>𝑺</p><p>+𝟏(𝟐)</p><p>In the context of convolutional operations,<em class='similar'> H and W represent the height and width of the input feature map,</em> respectively. K denotes the size of the convolutional kernel, such as 3×3. P refers to the padding applied to the input, and S indicates the stride, which determines the step size of the convolution across the input feature map.</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">Supervised Learning for Retinal Disease Classification using Fundus Images .pdf_第5部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>3.2.2 Activation function (ReLU):</p><p>VGG19 Each convolutional layer is followed by a ReLU activation function, which serves</p><p>to introduce nonlinearity. the ReLU is calculated as:</p><p>𝑹𝒆𝑳𝑼(𝒙)=𝒎𝒂𝒙(𝟎,𝒙)(𝟑)</p><p>3.2.3 Pooling layer (Max Pooling):</p><p><em class='similar'>The maximum pooling layer is used to downsample the feature map and is usually added after every two convolutional layers.</em><em class='similar'> Assuming </em>a pooling window size of P × P and a step</p><p>size of S, the output size after pooling is:</p><p>𝑯′=</p><p>𝑯−𝑷</p><p>𝑺</p><p>+𝟏(4)</p><p>𝑾′=</p><p>𝑾−𝑷</p><p>𝑺</p><p>+𝟏(𝟓)</p><p>In the pooling operation, the parameters H, W, P, and S keep the same definitions as in the convolutional <em class='similar'>layer,</em><em class='similar'> referring to the height and width of the input,</em> the pooling window size, and the step size, respectively.</p><p>3.3 ResNet</p><p>ResNet (Residual Neural Network) is a deep neural network proposed by a team from Microsoft Research in 2015 that solves the gradient vanishing problem by introducing Residual Blocks and Skip Connections. The Residual Block allows information to be passed directly between network layers, allowing gradients to flow smoothly to the shallow layers when backpropagating.[24] This innovative design not only improves the training effect of the model, but also allows ResNet to build very deep network structures (e.g.,152 layers) without affecting the performance due to gradient vanishing, and thus performs well in tasks such as image recognition and detection.</p><p>Figure 10 Architecture of ResNet50</p><p>3.3.1 Residual block (ResBlock)</p><p>The ‘Residual Blocks’ are the core components of ResNet. The formula for each</p><p>ResBlock is as follows:</p><p>𝒚=𝑹𝒆𝑳𝑼(𝑭(𝒙)+𝑰(𝒙))(𝟔)</p><p>where x and y are the input and output of ResBlock, respectively. The function F is a</p><p>combination of a 3×3 convolution with step size (s, s), batch normalization (BN), ReLU,</p><p>another 3×3 convolution with step size (1,1), and BN. The jump connection I depends on the dimension of F (x); if the dimension of x coincides with that of F (x), then I is the</p><p>identity; otherwise,<em class='similar'> I is a 1×1 convolution followed by BN,</em><em class='similar'> which adjusts the dimension of x to coincide with that of F </em><em class='similar'>(x).</em></p><p>Figure 11 Basic module of Residual learning [25]</p><p>3.4 Self-Supervised Learning and Supervised Learning</p><p>Supervised learning is a traditional machine learning approach in which models are trained using labeled datasets. Each input sample is associated with a corresponding target label, thus allowing the model to learn a direct mapping from input to output. Supervised learning is widely used in tasks such as image classification and regression, where its performance depends heavily on the availability of sufficient high-quality labeled data.</p><p>Self-supervised learning, on the other hand, is an approach that eliminates the need for manual labeling by generating supervised signals directly from the data itself. Models are trained to solve predefined pretexting tasks with the goal of learning meaningful feature representations from unlabeled data.<em class='similar'> Self-supervised learning is increasingly used in areas such as computer vision and natural language processing,</em><em class='similar'> especially when labeled</em></p><p>data is scarce or expensive to acquire.[26]</p><p>3.5 Squeeze-and-Excitation attention</p><p>The Squeeze-and-Excitation (SE) attention mechanism is a lightweight and efficient attention module characterized by a small number of parameters and a simple structure. It operates by first applying a pooling operation to compress the spatial dimensions of the input feature map, capturing channel-wise global information. The resulting descriptor is then passed through a fully connected layer to perform dimensionality reduction and expansion, enabling the model to learn inter-channel dependencies. At last, an activation function is used to generate a weight vector, which is applied to recalibrate the original</p><p>feature maps.[27] The structure of SE module is shown as below:</p><p>Figure 12 Structure of SE module[27]</p><p>3.6 Evaluation Metrics</p><p>In this section, the key evaluation metrics used to assess the accuracy and reliability of the model will be described. These metrics include: accuracy, loss (cross-entropy loss), recall, precision, and F1-score, each of them provides a different perspective on the model&#39;s predictive capabilities.</p><p>3.6.1 Accuracy:</p><p><em class='similar'>The accuracy rate indicates the proportion of samples correctly predicted by the model</em></p><p>and is given by the formula:</p><p>𝑨𝒄𝒄𝒖𝒓𝒂𝒄𝒚=</p><p>𝑻𝑷+𝑻𝑵</p><p>𝑻𝑷+𝑻𝑵+𝑭𝑷+𝑭𝑵</p><p>(𝟏)</p><p>TP: True Positives (true classes, number of positive classes correctly classified)</p><p>TN: True Negatives (True Negatives, number of negative classes correctly classified)</p><p>FP: False Positives (False Positive Classes, the number of negative classes that are</p><p>incorrectly classified as positive)</p><p>FN: False Negatives (False Negative Classes, number of positive classes misclassified</p><p>as negative)</p><p>3.6.2 Loss(Cross-Entropy Loss):</p><p>The loss value is a measure of the difference between the model predictions and the</p><p>true labels, and is given by the formula:</p><p>𝑳𝒐𝒔𝒔=−</p><p>𝑁</p><p>∑∑𝑦𝑖𝑗 log(𝑝𝑖𝑗)</p><p>𝐶</p><p>𝑗=1</p><p>𝑁</p><p>𝑖=1</p><p>(2)</p><p>N is the number of samples in the batch</p><p>C is the number of classes</p><p>𝒚𝒊𝒋 is the true label of the 𝑖 th sample in the 𝑗 th class (One-hot code)</p><p>𝒑𝒊𝒋 is the probability that the 𝑖 th sample predicted by the model belongs to class 𝑗</p><p>3.6.3 Recall:</p><p>Recall indicates the proportion of all actual positive samples that the model correctly</p><p>predicts as positive, with the formula:</p><p>𝑹𝒆𝒄𝒂𝒍𝒍=</p><p>𝑻𝑷</p><p>𝑻𝑷+𝑭𝑵</p><p>(𝟑)</p><p>Recall measures how many of the model&#39;s actual positive class samples are correctly classified as positive.</p><p>3.6.4 Precision:</p><p>Precision indicates the proportion of all samples predicted to be positive that are</p><p>positive. The formula:</p><p>𝑷𝒓𝒆𝒄𝒊𝒔𝒊𝒐𝒏=</p><p>𝑻𝑷</p><p>𝑻𝑷+𝑭𝑷</p><p>(𝟒)</p><p><em class='similar'>The higher the Precision,</em><em class='similar'> the better the accuracy of the model in predicting the positive class,</em> i.e., fewer false positives (FPs).</p><p>3.6.5 F1 Score:</p><p>The F1 Score is the reconciled average of Precision and Recall and is commonly used</p><p>in the evaluation of unbalanced datasets. The formula is:</p><p>𝑭𝟏𝑺𝒄𝒐𝒓𝒆=𝟐×</p><p>𝑷𝒓𝒆𝒄𝒊𝒔𝒊𝒐𝒏×𝑹𝒆𝒄𝒂𝒍𝒍𝑷𝒓𝒆𝒄𝒊𝒔𝒊𝒐𝒏+𝑹𝒆𝒄𝒂𝒍𝒍</p><p>(𝟓)</p><p>The value of F1 Score is between 0 and 1(or 0% to 100%), and the closer it is to 1, the better the model performance. F1 Score is a composite indicator, which is suitable for</p><p>evaluating the model performance when the distribution of positive and negative samples is not balanced.</p><p>3.7 Dataset</p><p>The dataset[a] contains a large number of retinal images taken using fundus photography under a variety of imaging conditions, and clinicians rated the severity of diabetic</p><p>retinopathy for each image on a scale of 0 to 4:</p><p>0- no DR,1- mild,2- moderate,3- severe, and 4- proliferative DR.</p><p>The dataset is provided in the form of a compressed package containing train.csv - the training labels, test.csv - the training labels, test.csv - the training labels, and test.csv - the training labels. train.csv - training labels, test.csv - test set, and train.zip - training set images (3662 PNG images), test.zip - public test set images (1928 PNG images).</p><p>Table 2 Overview of images from the APTOS 2019 dataset</p><p>Class Data</p><p>No DR 1805</p><p>Mild DR 370</p><p>Moderate DR 999</p><p>Severe DR 193</p><p>Proliferative DR 295</p><p>Total 3662</p><p>Figure 13 Example images from the APTOS 2019 dataset:</p><p>3.8 Dataset preprocessing</p><p>In this project, systematic preprocessing is performed for APTOS 2019 Blindness Detection dataset to improve the model training effect and generalization ability. The preprocessing is mainly divided into two parts: image cropping and enhancement processing, and dataset expansion.</p><p>3.8.1 Image Cropping and Enhancement</p><p>Raw retinal images often have a large amount of extraneous background (e.g., black regions), and the direct use of these images may lead to increased training noise and slower model convergence. Therefore, this study designs an image cropping method based on gray-level thresholding and contour detection to retain the effective regions and standardize the image size.</p><p>The specific process is as follows:</p><p>Gray-level Cropping: after converting the image into a grayscale map, a mask is generated based on the luminance threshold (set to 7), and only the regions with luminance higher than the threshold are retained, and then re-sampled according to the original size.</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">Supervised Learning for Retinal Disease Classification using Fundus Images .pdf_第6部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>Contour-based Cropping: Through binarization and contour extraction techniques, locate the largest outer rectangular region in the image and crop it accordingly.</p><p>Universal Cropping Interface: In order to be compatible with different strategies, a unified cropping function universal_crop is designed, which can be used to select grayscale cropping or contour cropping methods according to the settings.</p><p>The cropped image is further enhanced to simulate the multiple variations that may occur during the imaging process. The enhancement method adopts the Ben Graham style (Ben&#39;s preprocessing), which improves the local contrast of the image through Gaussian blurring and weighted superposition, and enhances the recognizability of key information such as blood vessels and exudates.</p><p>The entire cropping and enhancement process is integrated in the batch processing function enhance_and_save_images, which ensures that all training samples are processed under a uniform specification.</p><p>The comparison of the effects before and after preprocessing is presented in Figure 14:</p><p>Figure 14 Comparison of image cropping and enhancement results</p><p>3.8.2 Dataset Balancing</p><p>As the sample size of different lesion classes (Label 0-4) in the APTOS 2019 dataset is extremely unbalanced, the details can be seen in the dataset introduction. In order to alleviate the problem of category skewness, this study introduces the Data Augmentation (DA) technique to expand the categories with less sample size.</p><p>The augmentation process is as follows:</p><p>Firstly, the image files that have been processed (after cropping + augmentation) are read. Then, using ImageDataGenerator under the Keras framework, transformations such as rotation (±25°), random scaling (±15%), horizontal flipping, and brightness adjustment (0.8-1.2 times) are applied to generate diverse images. Then for each category, the target number of samples is set to be the number of samples of the category with the most samples in the current dataset. Finally, new images are dynamically generated on the basis of the original samples, saved to a specified directory, and the label file (CSV format) is synchronously updated.</p><p>In addition, in order to avoid repeated enhancement, the system checks the existing enhanced images before each expansion to ensure that the generation process is efficient and redundancy-free.</p><p>The following is the distribution of the expanded dataset:</p><p>Figure 15Comparison of dataset distribution before and after expansion</p><p>Chapter 4 Implementation and Result Analyses</p><p>4.1 The implementation of the Gu_CNN model</p><p>In this project, the core model architecture combines VGG convolutional feature extraction with residual layer of ResNet, while integrating a Squeeze-and-Excitation (SE) attention mechanism to enhance channel feature recalibration.</p><p>The overall structure starts with a series of convolutional and residual layers, followed by global pooling and a fully connected classification layer. This hybrid design aims to preserve fine-grained visual patterns in early stages while allowing deeper abstraction and stable gradient flow in later stages. By exploiting local feature sensitivity and global feature generalization, the architecture seeks to achieve high accuracy in diabetic retinopathy classification.</p><p>Figure 16 The general architecture diagram of the model</p><p>4.2 Initial Convolution and Residual Feature Learning</p><p>The model starts with a 3×3 convolutional layer and applies 64 filters on the input image followed by batch normalization and ReLU activation function. This initial convolution is used to extract basic low-level features such as edges and texture.</p><p>After that, the model enters multiple stages, each consisting of a standard convolution block, a custom residual block, and a MaxPooling layer. In each residual block, which architecture is shown in figure 17, two convolutional layers are superimposed, and each convolutional layer is followed by batch normalization. Residual concatenation is achieved by shortcuts that convert inputs directly to outputs, with the option of 1×1 convolution if the number of filters changes. In addition, an SE attention module is inserted between the convolution and addition operations, allowing the model to adaptively recalibrate the feature channel response.</p><p>Figure 17 Architecture of the Residual Block with SE Attention</p><p>4.3 Stage-wise Architecture Details</p><p>The detailed progression through the network is divided into four main phases.</p><p>In the first phase, a residual block with 64 filters is applied after the initial convolutional block, followed by a MaxPooling operation to reduce the spatial dimension by half.</p><p>The second phase continues with a convolutional block containing 128 filters, followed by another residual block containing 128 filters, and a subsequent MaxPooling layer.</p><p>The third stage involves two consecutive 3×3 convolutions with 256 filters, followed by a residual block with 256 filters. The MaxPooling layer is again used to downsample the feature mapping.</p><p>In the final stage, a convolution with 512 filters is applied, followed by a residual block with 512 filters. After this, MaxPooling is performed to further compress the spatial information. The gradual increase in the number of filters and the decrease in spatial resolution allows the model to efficiently capture the complex and hierarchical features required for fine-grained diabetic retinopathy classification.</p><p>The table of the architecture of the proposed model is shown as below:</p><p>Table 3 Architecture Overview of the Gu_CNN Model Across Different Stages</p><p>Stage Input Size Operation Filters Output Size</p><p>0224×224×3</p><p>Conv 3×3+ BN</p><p>+ ReLU</p><p>64224×224×64</p><p>0224×224×64</p><p>Residual Block</p><p>+ SE</p><p>64224×224×64</p><p>0224×224×64</p><p>MaxPooling</p><p>2×2</p><p>-112×112×64</p><p>1112×112×64</p><p>Conv 3×3+ BN</p><p>+ ReLU</p><p>128112×112×128</p><p>1112×112×128</p><p>Residual Block</p><p>+ SE</p><p>128112×112×128</p><p>1112×112×128</p><p>MaxPooling</p><p>2×2</p><p>-56×56×128</p><p>256×56×128</p><p>2×(Conv 3×3+</p><p>BN + ReLU)</p><p>25656×56×256</p><p>256×56×256</p><p>Residual Block</p><p>+ SE</p><p>25656×56×256</p><p>256×56×256</p><p>MaxPooling</p><p>2×2</p><p>-28×28×256</p><p>328×28×256</p><p>Conv 3×3+ BN</p><p>+ ReLU</p><p>51228×28×512</p><p>328×28×512</p><p>Residual Block</p><p>+ SE</p><p>51228×28×512</p><p>328×28×512</p><p>MaxPooling</p><p>2×2</p><p>-14×14×512</p><p>4.4 Global Pooling and Classification</p><p>After feature extraction, a Global Average Pooling (GAP) layer is applied to reduce each feature mapping to a single value by computing the average across spatial dimensions. This step not only reduces the number of parameters, but also strengthens the spatial invariance, which is required for classification tasks.</p><p>The output of the GAP layer is then passed through a fully connected (Dense) layer followed by a Softmax activation function. The final layer outputs five predicted probability distributions for diabetic retinopathy severity (ranging from 0 to 4).</p><p>4.5 Implementation Details</p><p><em class='similar'>The original dataset is divided into training,</em><em class='similar'> validation and testing subsets in the ratio of 8:</em><em class='similar'>1:1.</em> Initially,<em class='similar'>80% of the samples were assigned to the training set and the remaining 20% was further split evenly into the validation and test sets.</em> Stratified sampling was used during splitting to preserve the original class distribution of all subsets. The data size obtained was 7297 samples for training,912 samples for validation and 913 samples for testing.</p><p>The input dataset is processed before starting the training by normalizing all the pixel values to the range [0,1]. For the training set, various data enhancement techniques were applied using the imagedataggenerator class in TensorFlow. The enhancement operations include random rotation up to 20 degrees,<em class='similar'> horizontal and vertical shifts up to 20% of the image width or height,</em><em class='similar'> random clipping transformations,</em> scaling changes in the range of 20%, and horizontal flipping. For both the validation and test sets, only the pixel values are rescaled to ensure that the evaluation results accurately reflect real-world generalizations without artificial variations.</p><p>The model was trained for up to 100 epochs with a batch size of 16 and an input image size of 224×224 pixels. The initial learning rate was set to 5×10−4. An early stopping mechanism is also incorporated to stop training if the validation loss does not improve for 8 consecutive validations, and restores the model weights from the epoch that achieved the best validation performance.</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">Supervised Learning for Retinal Disease Classification using Fundus Images .pdf_第7部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>To dynamically adjust the learning rate during training, a CosineDecayRestarts scheduler was employed. In this schedule, the first decay cycle lasts 5 epochs, with each subsequent cycle doubling in length. After each cycle, the maximum learning rate decreases by a factor of 0.8. A small minimum learning rate (alpha=1e-6) was maintained to ensure continued learning during the later stages of training.</p><p>The optimizer used was Adam, enhanced with gradient clipping (clip norm of 1.0) to prevent gradient explosion, particularly in the presence of highly non-convex loss landscapes. The loss function applied was sparse categorical crossentropy, which is suitable for multi-class classification tasks with integer-encoded labels. Besides, the accuracy was used as the primary evaluation metric throughout training.</p><p>All the hyperparameters of this model are summarized in the following table:</p><p>Table 4 Summary of Hyperparameters</p><p>Hyperparameter Value/Description</p><p>Input Image Size 224×224 pixels</p><p>Batch Size 16</p><p>Optimizer Adam with gradient clipping</p><p>(clipnorm=1.0)</p><p>Initial Learning Rate 5×10⁻⁴</p><p>Learning Rate Scheduler Cosine Decay Restarts</p><p>First Decay Steps 10 epochs</p><p>Multiplicative Factor (t_mul)2.0</p><p>Learning Rate Decay Factor (m_mul)0.8</p><p>Minimum Learning Rate (alpha)1×10⁻⁶</p><p>Loss Function Sparse Categorical Crossentropy</p><p>Metric Accuracy</p><p>Early Stopping Patience 8 epochs without improvement in</p><p>validation loss</p><p>Class Weights Computed using Scikit-learn&#39;s</p><p>compute_class_weight based on training</p><p>set</p><p>Training/Validation/Test Split 80%/10%/10%, stratified sampling</p><p>Data Augmentation (Training) Rotation (20°), width/height shift (20%),</p><p>shear (0.2), zoom (0.2), horizontal flip</p><p>Data Augmentation (Validation/Test) Only rescaling to [0,1] range</p><p>Number of Epochs 100</p><p>4.6 Result</p><p>The Gu_CNN model was evaluated on the test set, achieving a test loss of 0.4642 and a test accuracy of 89.80%. During training, the model reached a training loss of 0.4624 with an accuracy of 91.38%, while on the validation set, the validation loss and accuracy were 0.4662 and 90.68%, respectively. The small gap between training and validation results indicates a consistent model behavior across different data splits.</p><p>The model&#39;s F1-score on the test set was 0.8988, with a recall of 0.8980 and a precision of 0.9026. The detailed classification report shows class-wise performance, where classes 0 and 4 exhibited relatively higher precision and recall, while classes 2 and 3 showed comparatively lower but stable metrics.</p><p>Table 5 Classification Report</p><p>precision</p><p>recall</p><p>f1-score support</p><p>00.930.970.95180</p><p>10.840.930.88180</p><p>20.790.810.80191</p><p>30.990.870.93181</p><p>40.960.920.94180</p><p>Accuracy 0.90912</p><p>Macro avg 0.900.900.90912</p><p>Weighted avg 0.900.900.90912</p><p>The confusion matrix is presented to further illustrate the model’s prediction distribution</p><p>across all classes:</p><p>Figure 18 Confusion Matrix of Gu_CNN on Aptos2019 dataset</p><p>From the confusion matrix, the model&#39;s prediction performance is excellent on most of the categories, and most of the samples are accurately categorized. Category 0(normal), category 1(mild DR), category 3(severe DR), and category 4(proliferative DR), the number of correct classifications dominated absolutely. It should be noted that there is a certain degree of confusion in category 2(moderate DR), with more misclassifications mainly with category 1, probably because these two categories are closer in terms of feature performance, which leads to an increase in the difficulty of model discrimination. Receiver Operating Characteristic (ROC) curves for each class were plotted to analyze</p><p>the sensitivity and specificity of the model at various thresholds:</p><p>Figure 19 ROC curves of Gu_CNN on Aptos2019 dataset</p><p>In the ROC curve analysis, the model shows very high differentiation ability for all categories, with AUC values higher than 0.98, among which the AUC of category 0 and category 4 reaches 1.00.<em class='similar'> The overall curves are close to the upper left corner,</em><em class='similar'> which indicates that the model has excellent sensitivity and specificity,</em><em class='similar'> and is able to differentiate between samples of different categories effectively.</em></p><p>In addition, Precision-Recall curves were generated to assess model performance under</p><p>class imbalance conditions:</p><p>Figure 20 Precision-Recall curves of Gu_CNN on Aptos2019 dataset</p><p>The stability of the model under positive and negative sample imbalance can be further verified by Precision-Recall (PR) curve analysis. The PR curves for category 0, category 1 and category 4 are close to the upper right corner, showing a good balance between high precision and high recall. Category 2 shows a certain magnitude of Precision decrease in the high recall interval, indicating an increase in the false positive rate when recalling moderate DR samples, which is consistent with the performance in the confusion</p><p>matrix, suggesting that category 2 can be optimized specifically for category 2 in the future, e.g., sample enhancement, category weight adjustment, etc.</p><p>Finally, the training process was monitored by recording the loss and accuracy curves over epochs, which provide insight into the model’s convergence behavior and training dynamics.</p><p>Figure 21 Training and Validation Loss(up) and Accuracy(down) curvesof Gu_CNN on</p><p>Aptos2019 dataset</p><p>The loss curves figure shows that the training loss (blue curve) starts at about 2.0 and gradually decreases and levels off as the number of training cycles increases, eventually stabilizing at about 0.5. The validation loss (yellow curve) is similarly high at first, at about 3.0, but declines more rapidly, decreasing rapidly over the first few cycles and then fluctuating between about 0.5 and 1.5, showing greater instability. A significant peak in validation loss occurs at about cycle 5, after which it declines but still shows large fluctuations throughout the training process.</p><p>The Accuracy curves figure on the right side shows the trend of the model&#39;s training accuracy and validation accuracy over 40 training cycles. It can be seen that the training accuracy (green curve) starts at about 0.70 and gradually rises and stabilizes as the training progresses, eventually approaching 0.95, showing that the model&#39;s performance on the training set continues to improve. The validation accuracy (red curve) starts low at about 0.55, but rises rapidly in the first few cycles and then fluctuates between 0.85 and 0.95, showing some instability. Despite the overall upward trend in validation accuracy, there are several significant decreases during training, especially around cycles 5,25, and 30.</p><p>4.7 Self-Supervised Model</p><p>To compare with the supervised model, a self-supervised model with a similar architecture is introduced. Although the self-supervised model shares many architectural similarities with the supervised model, it differs in the training approach, as the self-supervised model uses Masked Autoencoding (MAE) instead of the traditional label-based learning process.</p><p>4.7.1 Differences in Self-Supervised model with Supervised Model</p><p>Despite many similarities in the architectures of the self-supervised and supervised models, there are significant differences in their training methods and objectives. First, the self-supervised model utilizes the Masked Autoencoding (MAE) architecture, which involves randomly masking parts of the input image, forcing the model to learn how to reconstruct these missing parts and thereby learn useful image features. This process is fundamentally different from traditional supervised learning, where the model learns from labeled data.</p><p>In the design of the self-supervised model, the input image is randomly masked,<em class='similar'> and the masked image is fed into the network for encoding.</em><em class='similar'> The encoder extracts features from the image,</em><em class='similar'> and the decoder attempts to reconstruct the original image.</em><em class='similar'> The training process optimizes the model by minimizing the reconstruction error </em>(such as Mean Squared Error, MSE), rather than using traditional classification loss functions (such as Cross-Entropy). Unlike supervised models that focus on classification tasks, the self-supervised model&#39;s objective is to extract features from unlabeled data through self-learning.</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">Supervised Learning for Retinal Disease Classification using Fundus Images .pdf_第8部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>Additionally, the decoder structure in the self-supervised model differs from that of the supervised model. The output layer in supervised models is typically a fully connected layer that maps the features to the final class labels, whereas the output layer in the self-supervised model is a decoder that progressively restores the image details through</p><p>transposed convolutions, performing an image reconstruction task instead of classification.</p><p>4.7.2 Self-Supervised Model Results</p><p>The self-supervised model achieved an overall accuracy of 91%, demonstrating robust performance across various severity levels of diabetic retinopathy. It showed strong precision and recall in detecting both mild (Class 1) and severe (Class 3) diabetic retinopathy. The model&#39;s precision and recall ranged from 0.93 to 0.99, with macro-average precision, recall, and F1-score all reaching 0.94. While the model performed well overall, its performance was slightly weaker in detecting Class 2(moderate DR), where precision was lower compared to other classes. The specific classification result report is</p><p>presented in Table 6:</p><p>Table 6 Classification Report for Self-Supervised Model</p><p>precision</p><p>recall</p><p>f1-score support</p><p>00.930.970.95361</p><p>10.920.900.91361</p><p>20.780.920.84381</p><p>30.990.900.94361</p><p>40.980.860.92361</p><p>Accuracy 0.911825</p><p>Macro avg 0.920.910.911825</p><p>Weighted avg 0.920.910.911825</p><p>The accuracy and loss curves (figure 22) demonstrate the model&#39;s effective learning process. The accuracy curve shows a steady increase during training, reaching a peak around 91%, with minimal fluctuation between training and validation accuracy. The validation accuracy closely mirrors the training accuracy, indicating that the model generalizes well and avoids overfitting. In contrast, the loss curve steadily decreases over epochs, indicating that the model is successfully minimizing reconstruction error and learning the relevant features from the data. But unfortunately, the loss rate eventually</p><p>stabilized above 0.6</p><p>Figure 22 Training and Validation Loss(up) and Accuracy(down) curves of Self-supervised</p><p>model on Aptos2019 dataset</p><p>The confusion matrix (Figure 23) indicates that the self-supervised model performs excellently in identifying Class 0 and Class 3, with a few misclassifications occurring mainly between Classes 1,2, and 4. The majority of predictions fall on the diagonal, showing that the model correctly classifies most of the instances, though some errors remain due to the complexities of differentiating between similar classes.</p><p>Figure 23 Confusion Matrix of Self-supervised model on Aptos2019 dataset</p><p>The precision-recall curves (figure 24) shows that the self-supervised model performs well in maintaining high precision while ensuring a relatively high recall, especially for Class 0 and Class 3. The curves show that the model is particularly effective in detecting the absence of DR and severe DR, with area under the curve (AP) values above 0.90 for all classes.</p><p>Figure 24 Precision-Recall curves of Self-supervised model on Aptos2019 dataset</p><p>The multiclass ROC (figure 25) curve shows the model exhibits high performance for all classes, with an Area Under the Curve (AUC) close to 1.0 for all categories, especially for Class 0(no DR) and Class 1(mild DR).</p><p>Figure 25 ROC curves of Self-supervised model on Aptos2019 dataset</p><p>4.8 Result comparation</p><p>Besides of all the result shown in the first few sections, the training time of the self-supervised model was much longer than the supervised model. While the supervised model required approximately 6 to 8 hours for training, the self-supervised model took more than 12 hours to complete its training. When come in to performance, the self-supervised model achieved a slightly higher accuracy, with a difference of about 0.02, but a much higher loss rate of above 0.6, which was 0.1 higher than the supervised model&#39;s loss. Despite this, the self-supervised model was still a viable approach but less efficient in this project.</p><p>As short, due to the faster training time and an acceptable performance, the supervised model was selected to proceed with the classification task for diabetic retinopathy, ensuring a balance between efficiency and accuracy for real-world application.</p><p>4.9 Model Visualization</p><p>Grad-CAM (Gradient-weighted Class Activation Mapping) is a technique used to visualize the focus areas of Convolutional Neural Networks (CNNs) during classification, improving the interpretability of the model. It generates heatmaps that highlight regions in an image that the model considers important for making its prediction. In these heatmaps, blue areas represent regions with less focus, while red areas indicate where the model is paying the most attention.[28] In the provided figure 22 of diabetic retinopathy (DR), Grad-CAM shows how the model focuses on different areas depending on the severity of DR. For normal and mild DR images, the model’s attention is concentrated on the central and outer regions of the retina, where abnormalities are less visible. However, in moderate and severe DR images, the heatmaps reveal that the model focuses more on areas with significant lesions, such as exudates and hemorrhages, reflecting the model&#39;s ability to identify more prominent signs of the disease. This technique provides valuable insights into the model&#39;s decision-making process, increasing its transparency and helping users understand which features are most influential in the diagnosis.</p><p>Figure 26 Grad-CAM Visualization</p><p>4.10 GUI</p><p>Building upon the robust model architecture discussed earlier,<em class='similar'> a graphical user interface </em><em class='similar'>(GUI)</em><em class='similar'> was developed to facilitate user interaction with the diabetic retinopathy detection system.</em><em class='similar'> The GUI,</em><em class='similar'> which is shown in figure 23-25,</em><em class='similar'> serves as an </em>intuitive interface for healthcare professionals and other users to upload retinal images, initiate the classification process, and view results in real-time. By leveraging the trained Gu_CNN model, the GUI provides an easy-to-use platform for predicting the severity of diabetic retinopathy, allowing users to understand and visualize the model&#39;s output.</p><p>Figure 27 Home Page</p><p>Figure 28 Dataset shown in home page</p><p>By clicking on the &quot;DR Check&quot; button in the home page(Figure 23), the user can entre the DR check page. In the DR check page which is shown in figure25, the user can simply upload the fundus image by clicking on the green button &quot;Chose File&quot; and select. After that, with one click on the &quot;Start Checking&quot; button, the classification results will be displayed on the right side. Users can compare the results given on the web page to determine whether they have DR.</p><p>Figure 29 DR Check page</p><p>Chapter 5 Professional Issues</p><p>5.1 Project Management</p><p>This section outlines the various phases and activities that were undertaken to manage the project efficiently. From the initial preparation, where foundational knowledge was accumulated, to the final testing and reporting stages, the project was systematically</p><p>organized to meet its objectives. The following subsections will detail the specific activities, schedule, and data management practices that guided the project to its successful completion.</p><p>5.1.1 Activities</p><p>Table 7 Table of Activities</p><p>Phase Objectives Status 1. Preparation 1. Review DR deep learning.</p><p>2. Learn matrix operations, vector spaces, eigenvalues and eigenvectors, and basic</p><p>probability and statistics</p><p>3. Study classification</p><p>methods, Understanding the</p><p>difference between</p><p>supervised and unsupervised learning.</p><p>4. Understand the principles</p><p>of neurons, activation</p><p>functions, loss functions,</p><p>forward and backward propagation.</p><p>5. Learn the principles of</p><p>convolutional, pooling, and fully connected layers, and the application of CNNs in image processing.</p><p>Done</p><p>2. Accumulation of further</p><p>knowledge</p><p>1. Research how VGG19</p><p>and ResNet worked in image classification.</p><p>Done</p><p>2. Research how DR</p><p>conditions are classified at various levels.</p><p>3. Find and delve into at</p><p>least four CNN models (e.g.,</p><p>VGG16 and 19, variants of</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">Supervised Learning for Retinal Disease Classification using Fundus Images .pdf_第9部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>ResNet) and related</p><p>programming libraries that can be leveraged.</p><p>4. Learn the definitions,</p><p>principles and modifications</p><p>of loss functions, optimizers,</p><p>model building and</p><p>optimization.</p><p>5. Research into measures</p><p>such as GANs to assist in training models.</p><p>3. Data collection 1. Gather at least 3 DR datasets from Kaggle.</p><p>2. The dataset was classified</p><p>into 5 categories: No DR,</p><p>Mild, Moderate, Severe, Proliferative DR.</p><p>3. Setting multiple test set</p><p>and training set ratios</p><p>Done</p><p>4. Actual development and</p><p>implementation</p><p>1. Build VGG19&amp;ResNet mix model.</p><p>2. Trying to deepen the</p><p>model</p><p>3. Repeat training, analyze,</p><p>compare results and improve.</p><p>4. Optimize the selected model and continuously</p><p>adjust the hyperparameters until optimal.</p><p>Done</p><p>5. Final testing and other</p><p>work</p><p>1. Testing with other</p><p>datasets to detect</p><p>generalization ability.</p><p>Done</p><p>2. Summarize all results,</p><p>comparatively analyze and record.</p><p>3. Finish up Project Report</p><p>and Prepare for the</p><p>presentation</p><p>5.1.2 Schedule</p><p>The time schedule is shown below</p><p>In the table3 below,1-1 stands for Phase 1 Objective1,1-2 stands for Phase 1 Objective 2 et.al. All tasks are shown in table 6.</p><p>Figure 30 Gantt Chart</p><p>5.1.3 Project Data Management</p><p>Save all files (including: datasets, various reports {proposal, weekly report, progress report, final report}, thesis, drawn images as well as source files, different versions of the model code, test logs) on a laptop, desktop computer, GitHub, and USB stick respectively. Synchronize data on GitHub once a day, on the hard disk every two days, and sync all files to the desktop computer every Friday.</p><p>The GitHub link is: https://github.com/Blacknomizi/DR-detect</p><p>5.1.4 Project Deliverables</p><p>A. Project proposal B. Progress Report</p><p>C. Final Project Report</p><p>D. Project codes</p><p>E. Project presentation slides</p><p>F. Poster presentation file</p><p>5.2 Risk Analysis</p><p>In the process of developing and deploying the self-supervised learning model for retinal disease classification, several risks must be considered. These risks span across</p><p>technical, resource, and schedule aspects:</p><p>Technical Risks: Deep learning models for retinal disease classification, particularly those based on self-supervised learning, face substantial technical challenges. These include the difficulty of achieving high accuracy with limited labeled data, the risk of overfitting, and the need for fine-tuning model architectures to handle complex medical image data. Additionally, training such models requires significant computational resources, which may lead to long training times and high costs.</p><p>Resource Risks: Access to high-quality, annotated fundus images is crucial for training accurate models. However, medical datasets can be scarce and expensive to acquire. Moreover, there may be challenges in securing necessary computational resources, such as GPUs for training the model, which could affect both the progress and performance of the project.</p><p>Schedule Risk: The inherent uncertainty in model development and the potential delays in obtaining data may lead to schedule risks. Adjustments may be required in case of unforeseen difficulties, such as slow model convergence, hardware failures, or challenges in obtaining sufficient labeled data.</p><p>Mitigation Strategies: To mitigate technical risks, one approach could be to adopt pre-trained models and fine-tune them with the available data, which may reduce training time and improve model robustness. Resource risks can be addressed through collaboration with medical institutions or by leveraging publicly available datasets. To manage schedule risks, regular progress reviews and a flexible timeline should be implemented, ensuring the project can adapt to changes without significant delays.</p><p>Impact of Addressed Risks: Addressing these risks might lead to an optimized approach,</p><p>including the use of data augmentation techniques to reduce the dependency on large datasets, as well as the exploration of lightweight models to reduce computational demands.</p><p>Future Risks: Potential future risks include the emergence of new technologies that may require revisiting the model architecture, the need for continuous data collection to adapt to evolving disease patterns, and the possibility of research staff turnover. Establishing long-term collaborations and ensuring ongoing access to necessary resources will help mitigate these risks.</p><p>5.3 Professional Issues</p><p>Legal Issues: In the development of a self-supervised learning model for retinal disease classification, it is essential to comply with regulations regarding data privacy and security. Medical data, especially images of patients&#39; eyes, is sensitive and must adhere to laws such as GDPR (General Data Protection Regulation) or HIPAA (Health Insurance Portability and Accountability Act) to protect personal privacy. Furthermore, intellectual property issues related to the use of pre-trained models and datasets must be carefully managed.</p><p>Social Issues: The deployment of retinal disease classification systems powered by deep learning raises concerns about the ethical use of technology in medical diagnoses. Public perception of AI in healthcare can lead to concerns about job displacement for medical professionals and the reliability of AI-based decisions. These issues must be addressed by demonstrating the transparency and accountability of the AI system.</p><p>Ethical Issues: Ethical concerns are paramount when working with medical data. Informed consent from patients for the use of their medical images must be obtained, and data should only be used for the intended purposes. Additionally, fairness and equity must be considered to avoid biased model predictions, particularly in diverse populations. Steps should be taken to ensure that the model does not perpetuate or amplify existing healthcare disparities.</p><p>Environmental Issues: Training deep learning models requires substantial computational power, which can contribute to high energy consumption and environmental impact. To minimize the ecological footprint, the model development should prioritize energy-efficient computing resources, and consider offloading some computational tasks to cloud services that utilize renewable energy sources. Furthermore, regular evaluations of the environmental impact of training and model deployment should be conducted to ensure sustainability.</p><p>Chapter 6 Conclusion</p><p>This work proposed a CNN model for diabetic retinopathy (DR) classification, which combined VGG and ResNet architectures to extract low-level and high-level features, and feature recalibration using squeeze and excite (SE) attention. The model was trained on the preprocessed APTOS 2019 dataset (3,662 samples). The preprocessing techniques including image cropping, enhancement, class rebalancing, and data enhancement (rotation, scaling, and flipping). The model has a test accuracy of 89.80%, an F1 score of 0.8988, a precision of 0.9026, a recall of 0.8980, and a macro-mean of all metrics of 0.90. The training loss is 0.4624 and the validation accuracy is 90.68%. The confusion matrix showed strong categorization results, especially for severe and proliferative DR (grades 3 and 4), while the AUC values of the ROC curves were above 0.98 for all categories. The precision recall curves confirmed the balanced performance of the model, especially for the less represented categories. These results highlight the robustness of the model and reliable generalization across data subsets.</p><p>Although some achievements have been made, this study still has some limitations. A key issue is data imbalance, especially in severe and diffuse DR cases. Although data augmentation was used to solve this issue, the model showed lower performance in the moderate DR category, possibly due to subtle feature differences that make it difficult to distinguish between adjacent stages of DR, or overfitting due to a large number of expansions of the same fundus image. Since only one dataset was used, the reliance on the APTOS 2019 dataset may limit the model&#39;s ability to handle various imaging variations such as illumination, resolution, or other environmental factors. Further research is needed to evaluate the robustness of the model in different environments and with different data sources.</p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">Supervised Learning for Retinal Disease Classification using Fundus Images .pdf_第10部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p>Future work will be focused on improving the performance and adaptability of the model. Continue to improve the self-supervised learning models with better results and shorter training times, in order to achieve a further reduction in training costs by utilizing large amounts of unlabeled data. Additionally stronger regularization methods and optimized learning rate strategies will be considered to reduce losses. In addition, improved interpretability through explainable artificial intelligence (XAI) approaches to facilitate clinical deployment efforts, as healthcare professionals and general users need to trust and understand AI-driven predictions.</p><p class='uncheck'>References </p><p class='uncheck'>Papers: </p><p class='uncheck'>[1] Amin, Javeria, Sharif, Muhammad, Yasmin, Mussarat, A Review on Recent </p><p class='uncheck'>Developments for Detection of Diabetic Retinopathy, Scientifica, 2016, 6838976, 20 pages, 2016. https://doi.org/10.1155/2016/6838976. </p><p class='uncheck'>[2] M. Z. Atwany, A. H. Sahyoun and M. Yaqub, "Deep Learning Techniques for Diabetic </p><p class='uncheck'>Retinopathy Classification: A Survey," in IEEE Access, vol. 10, pp. 28642-28655, 2022, doi: 10.1M. Z. Atwany, A. H. Sahyoun and M. Yaqub, "Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey," in IEEE Access, vol. 10, pp. 28642-28655, 2022, doi: 10.1109/ACCESS.2022.3157632.109/ACCESS.2022.3157632. https://ieeexplore.ieee.org/document/9729867. </p><p class='uncheck'>[3] Y. Niu, L. Gu, Y. Zhao and F. Lu, "Explainable Diabetic Retinopathy Detection and Retinal Image Generation," in IEEE Journal of Biomedical and Health Informatics, vol. 26, no. 1, pp. 44-55, Jan. 2022, doi: 10.1109/JBHI.2021.3110593. https://ieeexplore.ieee.org/document/9531456. </p><p class='uncheck'>[4] S. Yang, J. Zhang, L. Chen. The cells involved in the pathological process of diabetic </p><p class='uncheck'>retinopathy, Biomedicine & Pharmacotherapy, Volume 132, 2020, 110818, ISSN 0753-3322, https://doi.org/10.1016/j.biopha.2020.110818. </p><p class='uncheck'>[5] Vora, P.; Shrestha, S. Detecting Diabetic Retinopathy Using Embedded Computer </p><p class='uncheck'>Vision. Appl. Sci. 2020, 10, 7274. https://doi.org/10.3390/app10207274 </p><p class='uncheck'>[6] N.H. Cho, J.E. Shaw, S. Karuranga, Y. Huang, J.D. da Rocha Fernandes, A.W. </p><p class='uncheck'>Ohlrogge, B. Malanda, IDF Diabetes Atlas: Global estimates of diabetes prevalence for 2017 and projections for 2045, Diabetes Research and Clinical Practice, Volume 138, 2018, Pages 271-281, ISSN 0168-8227, https://www.sciencedirect.com/science/article/pii/S0168822718302031. </p><p class='uncheck'>[7] S. Mishra, S. Hanchate and Z. Saquib, "Diabetic Retinopathy Detection using Deep </p><p class='uncheck'>Learning," 2020 International Conference on Smart Technologies in Computing, Electrical and Electronics (ICSTCEE), Bengaluru, India, 2020, pp. 515-520, doi: 10.1109/ICSTCEE49637.2020.9277506. </p><p class='uncheck'>https://ieeexplore.ieee.org/document/9277506 </p><p class='uncheck'>[8] S. Agarwal, A. Bhat, A survey on recent developments in diabetic retinopathy </p><p class='uncheck'>detection through integration of deep learning. Multimed Tools Appl 82, 17321-17351 </p><p class='uncheck'>(2023). https://doi.org/10.1007/s11042-022-13837-5 </p><p class='uncheck'>[9] C. Kabilan, S. Madhesh Kumar and G. Latha Selvi, "The Role of Fundus Imaging </p><p class='uncheck'>and GAN in Diabetic Retinopathy Classification using VGG19," 2024 International </p><p class='uncheck'>Conference on Advances in Computing, Communication and Applied Informatics </p><p class='uncheck'>(ACCAI), Chennai, India, 2024, pp. 1-5, doi: 10.1109/ACCAI61061.2024.10601890. </p><p class='uncheck'>https://ieeexplore.ieee.org/document/10601890 </p><p class='uncheck'>[10] R. S. Rajkumar, T. Jagathishkumar, D. Ragul and A. G. Selvarani, "Transfer </p><p class='uncheck'>Learning Approach for Diabetic Retinopathy Detection using Residual Network," 2021 6th International Conference on Inventive Computation Technologies (ICICT), Coimbatore, India, 2021, pp. 1189-1193, doi: 10.1109/ICICT50816.2021.9358468. https://ieeexplore.ieee.org/document/9358468. </p><p class='uncheck'>[11] Dai, L., Wu, L., Li, H. et al. A deep learning system for detecting diabetic retinopathy across the disease spectrum. Nat Commun 12, 3242 (2021). </p><p class='uncheck'>https://doi.org/10.1038/s41467-021-23458-5 </p><p class='uncheck'>[12] Simó, R., Stitt, A.W. & Gardner, T.W. Neurodegeneration in diabetic retinopathy: does it really matter?. Diabetologia 61, 1902-1912 (2018). </p><p class='uncheck'>https://doi.org/10.1007/s00125-018-4692-1 </p><p class='uncheck'>[13] Jo-Hsuan Wu, T Y Alvin Liu, Wan-Ting Hsu, Jennifer Hui-Chun Ho, Chien-Chang </p><p class='uncheck'>Lee, Performance and Limitation of Machine Learning Algorithms for Diabetic Retinopathy Screening: Meta-analysis, Journal of Medical Internet Research, Volume 23, Issue 7, 2021, ISSN 1438-8871, https://doi.org/10.2196/23863. </p><p class='uncheck'>[14] D. Yuan, J. Huang, X. Yang and J. Cui, "Improved random forest classification </p><p class='uncheck'>approach based on hybrid clustering selection," 2020 Chinese Automation Congress (CAC), Shanghai, China, 2020, pp. 1559-1563, doi: </p><p class='uncheck'>10.1109/CAC51589.2020.9326711. https://ieeexplore.ieee.org/document/9326711 </p><p class='uncheck'>[15] G. Priyadharshini and D. R. Judie Dolly, "Comparative Investigations on Tomato </p><p class='uncheck'>Leaf Disease Detection and Classification Using CNN, R-CNN, Fast R-CNN and Faster R-CNN," 2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS), Coimbatore, India, 2023, pp. 1540-1545, doi: 10.1109/ICACCS57279.2023.10112860. </p><p class='uncheck'>https://ieeexplore.ieee.org/document/10112860 </p><p class='uncheck'>[16] A. Vaish, D. Singh, A. Garg, A. Tiwari and A. Rathore, "Classification of Diabetic </p><p class='uncheck'>Retinopathy Using Improved ResNet-50 Deep Learning Model," 2024 Third International Conference on Distributed Computing and Electrical Circuits and Electronics (ICDCECE), Ballari, India, 2024, pp. 1-5, doi: 10.1109/ICDCECE60827.2024.10549184. </p><p class='uncheck'>https://ieeexplore.ieee.org/document/10549184 </p><p class='uncheck'>[17] A. He, T. Li, N. Li, K. Wang and H. Fu, "CABNet: Category Attention Block for </p><p class='uncheck'>Imbalanced Diabetic Retinopathy Grading," in IEEE Transactions on Medical Imaging, vol. 40, no. 1, pp. 143-153, Jan. 2021, doi: 10.1109/TMI.2020.3023463. https://ieeexplore.ieee.org/document/9195035. </p><p class='uncheck'>[18] G. Deshpande, Y. Govardhan and A. Jain, "Machine Learning-Based Diabetic </p><p class='uncheck'>Retinopathy Detection: A Comprehensive Study Using InceptionV3 Model," 2024 ASU International Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS), Manama, Bahrain, 2024, pp. 994-999, doi: 10.1109/ICETSIS61505.2024.10459541. </p><p class='uncheck'>https://ieeexplore.ieee.org/document/10459541. </p><p class='uncheck'>[19] A. Matthew, A. A. S. Gunawan and F. I. Kurniadi, "Diabetic Retinopathy Diagnosis </p><p class='uncheck'>System Based on Retinal Biomarkers Using EfficientNet-B0 for Android Devices," 2023 IEEE International Conference on Communication, Networks and Satellite (COMNETSAT), Malang, Indonesia, 2023, pp. 207-212, doi: 10.1109/COMNETSAT59769.2023.10420736. </p><p class='uncheck'>https://ieeexplore.ieee.org/document/10420736. </p><p class='uncheck'>[20] G. Deshpande, Y. Govardhan and A. Jain, "Machine Learning-Based Diabetic </p><p class='uncheck'>Retinopathy Detection: A Comprehensive Study Using InceptionV3 Model," 2024 ASU International Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS), Manama, Bahrain, 2024, pp. 994-999, doi: 10.1109/ICETSIS61505.2024.10459541. </p></p>
                </div>

                <h2 style="width:100%;text-align:center;margin-top:15px;">Supervised Learning for Retinal Disease Classification using Fundus Images .pdf_第11部分</h2>
                <h3 style="line-height:40px;">原文内容</h3>
                <div class="textOrig_con">
                    <p><p class='uncheck'>https://ieeexplore.ieee.org/document/10459541 </p><p class='uncheck'>[21] S. Dasari, B. Poonguzhali and M. Rayudu, "Transfer Learning Approach for </p><p class='uncheck'>Classification of Diabetic Retinopathy using Fine-Tuned ResNet50 Deep Learning Model," 2023 International Conference on Sustainable Communication Networks and Application (ICSCNA), Theni, India, 2023, pp. 1361-1367, doi: 10.1109/ICSCNA58489.2023.10370255. </p><p class='uncheck'>https://ieeexplore.ieee.org/document/10370255. </p><p class='uncheck'>[22] Y. Tao, "Image Style Transfer Based on VGG Neural Network Model," 2022 IEEE </p><p class='uncheck'>International Conference on Advances in Electrical Engineering and Computer Applications (AEECA), Dalian, China, 2022, pp. 1475-1482, doi: 10.1109/AEECA55500.2022.9918891. </p><p class='uncheck'>https://ieeexplore.ieee.org/document/9918891 </p><p class='uncheck'>[23] C. Kabilan, S. Madhesh Kumar and G. Latha Selvi, "The Role of Fundus Imaging </p><p class='uncheck'>and GAN in Diabetic Retinopathy Classification using VGG19," 2024 International </p><p class='uncheck'>Conference on Advances in Computing, Communication and Applied Informatics </p><p class='uncheck'>(ACCAI), Chennai, India, 2024, pp. 1-5, doi: 10.1109/ACCAI61061.2024.10601890. </p><p class='uncheck'>https://ieeexplore.ieee.org/document/10601890 </p><p class='uncheck'>[24] S. Dasari, B. Poonguzhali and M. Rayudu, "Transfer Learning Approach for Classification of Diabetic Retinopathy using Fine-Tuned ResNet50 Deep Learning Model," 2023 International Conference on Sustainable Communication Networks </p><p class='uncheck'>and Application (ICSCNA), Theni, India, 2023, pp. 1361-1367, doi: 10.1109/ICSCNA58489.2023.10370255. </p><p class='uncheck'>https://ieeexplore.ieee.org/document/10370255. </p><p class='uncheck'>[25] K. He, X. Zhang, S. Ren and J. Sun, "Deep Residual Learning for Image </p><p class='uncheck'>Recognition," 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 2016, pp. 770-778, doi: 10.1109/CVPR.2016.90. </p><p class='uncheck'>https://ieeexplore.ieee.org/document/7780459 </p><p class='uncheck'>[26] V. Deshpande, A. Sivanantham, I. Kapila, N. Singh, J. M and P. R, "Investigating </p><p class='uncheck'>The Potential of Self-Supervised Learning in Adversarial Machine Learning," 2024 IEEE 2nd International Conference on Innovations in High Speed Communication and Signal Processing (IHCSP), Bhopal, India, 2024, pp. 1-6, doi: 10.1109/IHCSP63227.2024.10959888. </p><p class='uncheck'>https://ieeexplore.ieee.org/document/10959888 </p><p class='uncheck'>[27] W. Xia, F. Yu, H. Wang and R. Hong, "A high-precision lightweight smoke </p><p class='uncheck'>detection model based on SE attention mechanism," 2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE), Guangzhou, China, 2022, pp. 941-944, doi: 10.1109/ICCECE54139.2022.9712739. </p><p class='uncheck'>https://ieeexplore.ieee.org/document/9712739 </p><p class='uncheck'>[28] T. He et al., "MediMLP: Using Grad-CAM to Extract Crucial Variables for Lung </p><p class='uncheck'>Cancer Postoperative Complication Prediction," in IEEE Journal of Biomedical and Health Informatics, vol. 24, no. 6, pp. 1762-1771, June 2020, doi: </p><p class='uncheck'>10.1109/JBHI.2019.2949601. https://ieeexplore.ieee.org/document/8883038 </p><p class='uncheck'>Dataset: </p><p class='uncheck'>[a]. Asia Pacific Tele-Ophthalmology Society (APTOS). APTOS 2019 Blindness </p><p>Detection. Available at: https://www.kaggle.com/c/aptos2019-blindness-</p><p>detection/data</p></p>
                </div>
              </div>
             <div class="report_explain2">
              <div class="repExp_left">说明：</div>
              <div class="repExp_rig">
                <p>1.指标是由系统根据《学术论文不端行为的界定标准》自动生成的</p>
                <p>2.本报告单仅对您所选择比对资源范围内检测结果负责</p>
              </div>
            </div>
            <div class="clear"></div>
            <div class="report_footer">
                <div class="assist_tool">
                  <h2>写作辅助工具</h2>
                  <ul>
                    <li>
                      <div class="asst icons asst1"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/recommendtitle/">选题分析</a>
                        <p>帮您选择合适的论文题目</p>
                      </div>
                    </li>
                    <li>
                      <div class="asst icons asst2"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/recommenddata/">资料搜集</a>
                        <p>提供最全最好的参考文章</p>
                      </div>
                    </li>
                    <li>
                      <div class="asst icons asst3"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/recommendoutline/">提纲推荐</a>
                        <p>辅助生成文章大纲</p>
                      </div>
                    </li>
                    <li>
                      <div class="asst icons asst4"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/writing/">在线写作</a>
                        <p>规范写作，提供灵感</p>
                      </div>
                    </li>
                    <li class="bgNo">
                      <div class="asst icons asst5"></div>
                      <div class="asstRig"> <a target="_blank" href="https://www.bigan.net/reference/">参考文献</a>
                        <p>规范参考文献，查漏补缺</p>
                      </div>
                    </li>
                  </ul>
                </div>
                <div class="repFot_bot">
                  <div class="reportCopy inlineBlock">版权所有：笔杆 www.bigan.net</div>
                  <div class="shareTo inlineBlock"><span>分享到：</span> <a href="http://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=https%3A%2F%2Fwww.bigan.net&title=%E5%92%8C%E6%88%91%E4%B8%80%E8%B5%B7%EF%BC%8C%E6%8B%BF%E8%B5%B7%E7%AC%94%E6%9D%86%EF%BC%8C%E5%81%9A%E4%B8%80%E4%B8%AA%E5%BF%AB%E4%B9%90%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%B8%9D%E3%80%82&summary=%E7%AC%94%E6%9D%86%EF%BC%8C%E4%B8%80%E4%B8%AA%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9C%A8%E7%BA%BF%E5%86%99%E4%BD%9C%E5%B9%B3%E5%8F%B0%E3%80%82%E7%AC%94%E6%9D%86%E4%B8%BA%E4%BD%A0%E6%89%AB%E6%B8%85%E5%86%99%E4%BD%9C%E6%80%9D%E8%B7%AF%E7%9A%84%E9%98%B4%E9%9C%BE%EF%BC%8C%E6%89%BE%E5%88%B0%E6%89%8D%E6%80%9D%E6%B3%89%E6%B6%8C%EF%BC%8C%E7%81%B5%E6%84%9F%E8%BF%B8%E5%8F%91%E7%9A%84%E5%BF%AB%E6%84%9F%EF%BC%8C%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BF%A1%E6%89%8B%E6%8B%88%E6%9D%A5%E3%80%82www.bigan.net&pics=https%3A%2F%2Fwww.bigan.net%2Flogo_80_80.png" target="_blank" title="QQ空间" class="inlineBlock sitem1 icons pngfix"></a> <a href="#" title="微信" class="inlineBlock sitem2 icons pngfix"></a> <a href="http://service.weibo.com/share/share.php?url=https%3A%2F%2Fwww.bigan.net&title=%E5%92%8C%E6%88%91%E4%B8%80%E8%B5%B7%EF%BC%8C%E6%8B%BF%E8%B5%B7%E7%AC%94%E6%9D%86%EF%BC%8C%E5%81%9A%E4%B8%80%E4%B8%AA%E5%BF%AB%E4%B9%90%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%B8%9D%E3%80%82%EF%BC%88%E7%AC%94%E6%9D%86%EF%BC%8C%E4%B8%80%E4%B8%AA%E4%B8%93%E4%B8%9A%E7%9A%84%E5%9C%A8%E7%BA%BF%E5%86%99%E4%BD%9C%E5%B9%B3%E5%8F%B0%E3%80%82%E7%AC%94%E6%9D%86%E4%B8%BA%E4%BD%A0%E6%89%AB%E6%B8%85%E5%86%99%E4%BD%9C%E6%80%9D%E8%B7%AF%E7%9A%84%E9%98%B4%E9%9C%BE%EF%BC%8C%E6%89%BE%E5%88%B0%E6%89%8D%E6%80%9D%E6%B3%89%E6%B6%8C%EF%BC%8C%E7%81%B5%E6%84%9F%E8%BF%B8%E5%8F%91%E7%9A%84%E5%BF%AB%E6%84%9F%EF%BC%8C%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BF%A1%E6%89%8B%E6%8B%88%E6%9D%A5%E3%80%82%40%E7%AC%94%E6%9D%86%E7%BD%91%EF%BC%89" target="_blank" title="新浪微博" class="inlineBlock sitem3 icons pngfix"></a> </div>
                </div>
              </div>
           </div>
       </div>
  </div>
</div>
</div>
</body>
</html>